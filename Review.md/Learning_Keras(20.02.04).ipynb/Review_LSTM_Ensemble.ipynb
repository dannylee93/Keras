{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   53000    53900    51800    51900 39565391]\n",
      " [   52600    53200    51900    52600 23104720]\n",
      " [   52600    52800    50900    50900 16128305]\n",
      " ...\n",
      " [   59100    59700    58800    59100 16446102]\n",
      " [   58800    58800    56800    57200 20821939]\n",
      " [   57800    58400    56400    56400 19749457]]\n",
      "[[   320.56    321.      316.75    316.75 173911.  ]\n",
      " [   317.79    319.53    314.95    315.87 141252.  ]\n",
      " [   315.93    316.08    312.6     314.42 123610.  ]\n",
      " ...\n",
      " [   294.38    295.67    292.45    293.98  85731.  ]\n",
      " [   293.27    294.11    287.09    288.37 101535.  ]\n",
      " [   290.24    291.47    284.53    284.53 101455.  ]]\n",
      "[[   53000    53900    51800    51900 39565391]\n",
      " [   52600    53200    51900    52600 23104720]\n",
      " [   52600    52800    50900    50900 16128305]\n",
      " [   51700    51700    50600    51600 13905263]\n",
      " [   52000    52200    51200    51300 10314997]] \n",
      " [50100]\n",
      "[[   320.56    321.      316.75    316.75 173911.  ]\n",
      " [   317.79    319.53    314.95    315.87 141252.  ]\n",
      " [   315.93    316.08    312.6     314.42 123610.  ]\n",
      " [   316.51    317.34    315.11    317.31 120900.  ]\n",
      " [   318.03    319.7     317.86    318.51 112400.  ]] \n",
      " [317.72]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "samsung = np.load(\"samsung.npy\")\n",
    "kospi200 = np.load(\"kospi200.npy\")\n",
    "\n",
    "print(samsung)\n",
    "print(kospi200)\n",
    "\n",
    "##########################################################################################\n",
    "               \n",
    "def split_xy(dataset, n_steps, y_column):\n",
    "    x,y = list(), list()\n",
    "    for i in range(len(dataset)):     \n",
    "        x_end_number = i + n_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        if y_end_number > len(dataset):  \n",
    "            break\n",
    "            \n",
    "        tmp_x, tmp_y = dataset[i:x_end_number, : ], dataset[x_end_number:y_end_number, 3] \n",
    "        x.append(tmp_x)                                      \n",
    "        y.append(tmp_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x1, y1 = split_xy(samsung,5,1)\n",
    "x2, y2 = split_xy(kospi200,5,1)\n",
    "print(x1[0,:], '\\n',y1[0])\n",
    "print(x2[0,:], '\\n',y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421, 5, 5) (421, 1) (421, 5, 5) (421, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape, y1.shape, x2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1,y1, \n",
    "                                                   random_state=1,\n",
    "                                                    test_size=0.6,\n",
    "                                                    shuffle=False)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2,y2, \n",
    "                                                   random_state=1,\n",
    "                                                    test_size=0.6,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x1_train = np.reshape(x1_train, (x1_train.shape[0], x1_train.shape[1] * x1_train.shape[2]))\n",
    "x1_test = np.reshape(x1_test, (x1_test.shape[0], x1_test.shape[1] * x1_test.shape[2]))\n",
    "                          \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x1_train)\n",
    "\n",
    "x1_train = scaler.transform(x1_train) \n",
    "x1_test = scaler.transform(x1_test)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "x2_train = np.reshape(x2_train, (x2_train.shape[0], x2_train.shape[1] * x2_train.shape[2]))\n",
    "x2_test = np.reshape(x2_test, (x2_test.shape[0], x2_test.shape[1] * x2_test.shape[2]))\n",
    "                          \n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(x2_train)\n",
    "\n",
    "x2_train = scaler.transform(x2_train) \n",
    "x2_test = scaler.transform(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 25) (168, 1) (168, 25) (168, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x1_train.shape, y1_train.shape, x2_train.shape, y2_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다시 Reshape (DNN 들어갈 모델 / LSTM 들어갈 모델)\n",
    "x1_train = np.reshape(x1_train, (x1_train.shape[0], 5, 5))\n",
    "x1_test = np.reshape(x1_test, (x1_test.shape[0], 5, 5))\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "x2_train = np.reshape(x2_train, (x2_train.shape[0], 5, 5))\n",
    "x2_test = np.reshape(x2_test, (x2_test.shape[0], 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 5, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           4864        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32)           4864        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           528         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           528         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            136         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            136         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            9           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            9           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           dense_3[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 8)            24          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            9           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,107\n",
      "Trainable params: 11,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#2. 모델 구성\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "input_1 = layers.Input(shape=(5,5))\n",
    "input_tensor_1 = layers.LSTM(32, activation='tanh')(input_1)   # x1에 투입할 모델\n",
    "hidden_layer_1 = layers.Dense(16, activation='relu')(input_tensor_1)\n",
    "hidden_layer_1 = layers.Dense(8, activation='relu')(hidden_layer_1)\n",
    "output_tensor_1 = layers.Dense(1, activation='relu')(hidden_layer_1)\n",
    "\n",
    "input_2 = layers.Input(shape=(5,5))\n",
    "input_tensor_2 = layers.LSTM(32, activation='tanh')(input_2)    # x2에 투입할 모델\n",
    "hidden_layer_2 = layers.Dense(16, activation='relu')(input_tensor_2)\n",
    "hidden_layer_2 = layers.Dense(8, activation='relu')(hidden_layer_2)\n",
    "output_tensor_2 = layers.Dense(1, activation='relu')(hidden_layer_2)\n",
    "\n",
    "from keras.layers.merge import concatenate , Add\n",
    "\n",
    "merged_model = concatenate(inputs=[output_tensor_1, output_tensor_2])  # 두 개 이상은 []로 묶기\n",
    "# merged_model = Add()([output_tensor_1, output_tensor_2])  # concatenate 대신 Add를 사용해도 가능하다.\n",
    "\n",
    "output_tensor_3 = layers.Dense(8)(merged_model)        # 첫 번째 아웃풋 모델\n",
    "output_tensor_3 = layers.Dense(1)(output_tensor_3)\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_1, input_2],\n",
    "              outputs=output_tensor_3)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 2021722944.7619 - mae: 44837.7344\n",
      "Epoch 2/100\n",
      "168/168 [==============================] - 0s 963us/step - loss: 2021708684.1905 - mae: 44837.5703\n",
      "Epoch 3/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 2021691332.5714 - mae: 44837.3828\n",
      "Epoch 4/100\n",
      "168/168 [==============================] - 0s 930us/step - loss: 2021670220.9524 - mae: 44837.1406\n",
      "Epoch 5/100\n",
      "168/168 [==============================] - 0s 929us/step - loss: 2021645239.6190 - mae: 44836.8633\n",
      "Epoch 6/100\n",
      "168/168 [==============================] - 0s 990us/step - loss: 2021616220.9524 - mae: 44836.5430\n",
      "Epoch 7/100\n",
      "168/168 [==============================] - 0s 843us/step - loss: 2021583148.1905 - mae: 44836.1680\n",
      "Epoch 8/100\n",
      "168/168 [==============================] - 0s 816us/step - loss: 2021545857.5238 - mae: 44835.7578\n",
      "Epoch 9/100\n",
      "168/168 [==============================] - 0s 803us/step - loss: 2021504479.2381 - mae: 44835.2969\n",
      "Epoch 10/100\n",
      "168/168 [==============================] - 0s 821us/step - loss: 2021459010.2857 - mae: 44834.7891\n",
      "Epoch 11/100\n",
      "168/168 [==============================] - 0s 945us/step - loss: 2021409500.1905 - mae: 44834.2422\n",
      "Epoch 12/100\n",
      "168/168 [==============================] - 0s 843us/step - loss: 2021355855.2381 - mae: 44833.6406\n",
      "Epoch 13/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2021298172.1905 - mae: 44832.9922\n",
      "Epoch 14/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2021236371.8095 - mae: 44832.3047\n",
      "Epoch 15/100\n",
      "168/168 [==============================] - 0s 833us/step - loss: 2021170653.7143 - mae: 44831.5703\n",
      "Epoch 16/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2021100852.5714 - mae: 44830.7969\n",
      "Epoch 17/100\n",
      "168/168 [==============================] - 0s 796us/step - loss: 2021027182.4762 - mae: 44829.9766\n",
      "Epoch 18/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2020949702.8571 - mae: 44829.1055\n",
      "Epoch 19/100\n",
      "168/168 [==============================] - 0s 802us/step - loss: 2020868185.1429 - mae: 44828.2031\n",
      "Epoch 20/100\n",
      "168/168 [==============================] - 0s 813us/step - loss: 2020782958.4762 - mae: 44827.2500\n",
      "Epoch 21/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2020693799.6190 - mae: 44826.2578\n",
      "Epoch 22/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2020601048.3810 - mae: 44825.2188\n",
      "Epoch 23/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2020504530.2857 - mae: 44824.1445\n",
      "Epoch 24/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2020404436.5714 - mae: 44823.0273\n",
      "Epoch 25/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2020300505.1429 - mae: 44821.8711\n",
      "Epoch 26/100\n",
      "168/168 [==============================] - 0s 885us/step - loss: 2020193339.4286 - mae: 44820.6719\n",
      "Epoch 27/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2020082435.0476 - mae: 44819.4336\n",
      "Epoch 28/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2019968023.6190 - mae: 44818.1562\n",
      "Epoch 29/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2019850035.0476 - mae: 44816.8398\n",
      "Epoch 30/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2019728754.2857 - mae: 44815.4883\n",
      "Epoch 31/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2019604060.9524 - mae: 44814.1016\n",
      "Epoch 32/100\n",
      "168/168 [==============================] - 0s 819us/step - loss: 2019475883.4286 - mae: 44812.6680\n",
      "Epoch 33/100\n",
      "168/168 [==============================] - 0s 843us/step - loss: 2019344656.7619 - mae: 44811.1992\n",
      "Epoch 34/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2019209775.2381 - mae: 44809.6953\n",
      "Epoch 35/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2019071849.1429 - mae: 44808.1602\n",
      "Epoch 36/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2018930665.9048 - mae: 44806.5898\n",
      "Epoch 37/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2018786337.5238 - mae: 44804.9805\n",
      "Epoch 38/100\n",
      "168/168 [==============================] - 0s 873us/step - loss: 2018638710.8571 - mae: 44803.3281\n",
      "Epoch 39/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2018488045.7143 - mae: 44801.6445\n",
      "Epoch 40/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2018334313.1429 - mae: 44799.9219\n",
      "Epoch 41/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2018177508.5714 - mae: 44798.1758\n",
      "Epoch 42/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2018017611.4286 - mae: 44796.3945\n",
      "Epoch 43/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2017854723.0476 - mae: 44794.5742\n",
      "Epoch 44/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2017689014.8571 - mae: 44792.7266\n",
      "Epoch 45/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2017520454.8571 - mae: 44790.8438\n",
      "Epoch 46/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2017348706.2857 - mae: 44788.9219\n",
      "Epoch 47/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2017174236.9524 - mae: 44786.9766\n",
      "Epoch 48/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2016996782.4762 - mae: 44785.0000\n",
      "Epoch 49/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2016816230.0952 - mae: 44782.9844\n",
      "Epoch 50/100\n",
      "168/168 [==============================] - 0s 796us/step - loss: 2016633254.8571 - mae: 44780.9375\n",
      "Epoch 51/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2016447081.1429 - mae: 44778.8633\n",
      "Epoch 52/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2016258460.9524 - mae: 44776.7578\n",
      "Epoch 53/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 2016066999.6190 - mae: 44774.6172\n",
      "Epoch 54/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2015872614.8571 - mae: 44772.4492\n",
      "Epoch 55/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2015675943.6190 - mae: 44770.2422\n",
      "Epoch 56/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2015476128.0000 - mae: 44768.0273\n",
      "Epoch 57/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2015274225.5238 - mae: 44765.7617\n",
      "Epoch 58/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2015069013.3333 - mae: 44763.4766\n",
      "Epoch 59/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2014861572.5714 - mae: 44761.1562\n",
      "Epoch 60/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2014651567.2381 - mae: 44758.8047\n",
      "Epoch 61/100\n",
      "168/168 [==============================] - 0s 796us/step - loss: 2014438868.5714 - mae: 44756.4297\n",
      "Epoch 62/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2014223846.0952 - mae: 44754.0273\n",
      "Epoch 63/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2014006005.3333 - mae: 44751.5938\n",
      "Epoch 64/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2013785717.3333 - mae: 44749.1367\n",
      "Epoch 65/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2013563014.0952 - mae: 44746.6445\n",
      "Epoch 66/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2013337896.3810 - mae: 44744.1289\n",
      "Epoch 67/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2013109930.6667 - mae: 44741.5781\n",
      "Epoch 68/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2012880003.8095 - mae: 44739.0078\n",
      "Epoch 69/100\n",
      "168/168 [==============================] - 0s 825us/step - loss: 2012647481.1429 - mae: 44736.4141\n",
      "Epoch 70/100\n",
      "168/168 [==============================] - 0s 801us/step - loss: 2012412562.2857 - mae: 44733.7891\n",
      "Epoch 71/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2012175172.5714 - mae: 44731.1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 2011935457.5238 - mae: 44728.4492\n",
      "Epoch 73/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2011693275.4286 - mae: 44725.7539\n",
      "Epoch 74/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2011449059.0476 - mae: 44723.0078\n",
      "Epoch 75/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2011201637.3333 - mae: 44720.2578\n",
      "Epoch 76/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2010952697.9048 - mae: 44717.4688\n",
      "Epoch 77/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2010700913.5238 - mae: 44714.6562\n",
      "Epoch 78/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2010447323.4286 - mae: 44711.8086\n",
      "Epoch 79/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2010190704.7619 - mae: 44708.9531\n",
      "Epoch 80/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2009932465.5238 - mae: 44706.0547\n",
      "Epoch 81/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2009671745.5238 - mae: 44703.1328\n",
      "Epoch 82/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 2009408515.0476 - mae: 44700.2031\n",
      "Epoch 83/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 2009143547.4286 - mae: 44697.2344\n",
      "Epoch 84/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2008876156.1905 - mae: 44694.2422\n",
      "Epoch 85/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2008606257.5238 - mae: 44691.2266\n",
      "Epoch 86/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2008334058.6667 - mae: 44688.1797\n",
      "Epoch 87/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 2008059786.6667 - mae: 44685.1250\n",
      "Epoch 88/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2007783728.7619 - mae: 44682.0234\n",
      "Epoch 89/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2007504870.8571 - mae: 44678.9102\n",
      "Epoch 90/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2007224492.1905 - mae: 44675.7578\n",
      "Epoch 91/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2006941321.1429 - mae: 44672.5938\n",
      "Epoch 92/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2006656650.6667 - mae: 44669.4062\n",
      "Epoch 93/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2006369488.7619 - mae: 44666.1953\n",
      "Epoch 94/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2006080360.3810 - mae: 44662.9453\n",
      "Epoch 95/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2005788848.7619 - mae: 44659.6992\n",
      "Epoch 96/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 2005495516.1905 - mae: 44656.4258\n",
      "Epoch 97/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 2005200102.0952 - mae: 44653.1172\n",
      "Epoch 98/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 2004902611.8095 - mae: 44649.7578\n",
      "Epoch 99/100\n",
      "168/168 [==============================] - 0s 766us/step - loss: 2004602764.9524 - mae: 44646.3945\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - 0s 766us/step - loss: 2004300764.1905 - mae: 44643.0391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x143627b10b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. 훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model.fit([x1_train, x2_train], y1_train, epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/253 [==============================] - 0s 908us/step\n",
      "results_acc:  [2297341512.347826, 47702.2734375]\n"
     ]
    }
   ],
   "source": [
    "results_acc = model.evaluate([x1_test, x2_test], y1_test, batch_size=5)\n",
    "print('results_acc: ', results_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [253, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-98e461873284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mRMSE_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE_mean :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSE_mean\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-98e461873284>\u001b[0m in \u001b[0;36mRMSE\u001b[1;34m(y_test, y_predict)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mRMSE_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 252\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 212\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [253, 1]"
     ]
    }
   ],
   "source": [
    "# RMSE 만들기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_predict = model.predict([x1_test,x2_test], batch_size=1)\n",
    "\n",
    "\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "\n",
    "RMSE_mean = (RMSE(y1_test, y_predict[0]) + RMSE(y2_test, y_predict[1])) / 2\n",
    "print(\"RMSE_mean :\", RMSE_mean )\n",
    "\n",
    "# R2 지표 만들기\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_1  = r2_score(y1_test, y_predict[0])\n",
    "r2_2  = r2_score(y2_test, y_predict[1])\n",
    "r2_mean  = (r2_1 + r2_2) / 2\n",
    "\n",
    "print(\"R2 : \", r2_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"종가(test) : {}\".format(y1_test[i]))\n",
    "    print(\"종가(pred) : {}\".format(y_predict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
