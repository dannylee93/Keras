{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   53000    53900    51800    51900 39565391]\n",
      " [   52600    53200    51900    52600 23104720]\n",
      " [   52600    52800    50900    50900 16128305]\n",
      " ...\n",
      " [   59100    59700    58800    59100 16446102]\n",
      " [   58800    58800    56800    57200 20821939]\n",
      " [   57800    58400    56400    56400 19749457]]\n",
      "[[   320.56    321.      316.75    316.75 173911.  ]\n",
      " [   317.79    319.53    314.95    315.87 141252.  ]\n",
      " [   315.93    316.08    312.6     314.42 123610.  ]\n",
      " ...\n",
      " [   294.38    295.67    292.45    293.98  85731.  ]\n",
      " [   293.27    294.11    287.09    288.37 101535.  ]\n",
      " [   290.24    291.47    284.53    284.53 101455.  ]]\n",
      "[[   53000    53900    51800    51900 39565391]\n",
      " [   52600    53200    51900    52600 23104720]\n",
      " [   52600    52800    50900    50900 16128305]\n",
      " [   51700    51700    50600    51600 13905263]\n",
      " [   52000    52200    51200    51300 10314997]] \n",
      " [50100]\n",
      "[[   320.56    321.      316.75    316.75 173911.  ]\n",
      " [   317.79    319.53    314.95    315.87 141252.  ]\n",
      " [   315.93    316.08    312.6     314.42 123610.  ]\n",
      " [   316.51    317.34    315.11    317.31 120900.  ]\n",
      " [   318.03    319.7     317.86    318.51 112400.  ]] \n",
      " [317.72]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "samsung = np.load(\"samsung.npy\")\n",
    "kospi200 = np.load(\"kospi200.npy\")\n",
    "\n",
    "print(samsung)\n",
    "print(kospi200)\n",
    "\n",
    "##########################################################################################\n",
    "               \n",
    "def split_xy(dataset, n_steps, y_column):\n",
    "    x,y = list(), list()\n",
    "    for i in range(len(dataset)):     \n",
    "        x_end_number = i + n_steps\n",
    "        y_end_number = x_end_number + y_column\n",
    "        if y_end_number > len(dataset):  \n",
    "            break\n",
    "            \n",
    "        tmp_x, tmp_y = dataset[i:x_end_number, : ], dataset[x_end_number:y_end_number, 3] \n",
    "        x.append(tmp_x)                                      \n",
    "        y.append(tmp_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x1, y1 = split_xy(samsung,5,1)\n",
    "x2, y2 = split_xy(kospi200,5,1)\n",
    "print(x1[0,:], '\\n',y1[0])\n",
    "print(x2[0,:], '\\n',y2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421, 5, 5) (421, 1) (421, 5, 5) (421, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape, y1.shape, x2.shape, y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1,y1, \n",
    "                                                   random_state=1,\n",
    "                                                    test_size=0.6,\n",
    "                                                    shuffle=False)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2,y2, \n",
    "                                                   random_state=1,\n",
    "                                                    test_size=0.6,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x1_train = np.reshape(x1_train, (x1_train.shape[0], x1_train.shape[1] * x1_train.shape[2]))\n",
    "x1_test = np.reshape(x1_test, (x1_test.shape[0], x1_test.shape[1] * x1_test.shape[2]))\n",
    "                          \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x1_train)\n",
    "\n",
    "x1_train = scaler.transform(x1_train) \n",
    "x1_test = scaler.transform(x1_test)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "x2_train = np.reshape(x2_train, (x2_train.shape[0], x2_train.shape[1] * x2_train.shape[2]))\n",
    "x2_test = np.reshape(x2_test, (x2_test.shape[0], x2_test.shape[1] * x2_test.shape[2]))\n",
    "                          \n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(x2_train)\n",
    "\n",
    "x2_train = scaler.transform(x2_train) \n",
    "x2_test = scaler.transform(x2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 25) (168, 1) (168, 25) (168, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x1_train.shape, y1_train.shape, x2_train.shape, y2_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다시 Reshape (DNN 들어갈 모델 / LSTM 들어갈 모델)\n",
    "x1_train = np.reshape(x1_train, (x1_train.shape[0], 5, 5))\n",
    "x1_test = np.reshape(x1_test, (x1_test.shape[0], 5, 5))\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "x2_train = np.reshape(x2_train, (x2_train.shape[0], 5, 5))\n",
    "x2_test = np.reshape(x2_test, (x2_test.shape[0], 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 5, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           4864        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32)           4864        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           528         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           528         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            136         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            136         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            9           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            9           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2)            0           dense_3[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 8)            24          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            9           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,107\n",
      "Trainable params: 11,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#2. 모델 구성\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "\n",
    "input_1 = layers.Input(shape=(5,5))\n",
    "input_tensor_1 = layers.LSTM(32, activation='tanh')(input_1)   # x1에 투입할 모델\n",
    "hidden_layer_1 = layers.Dense(16, activation='relu')(input_tensor_1)\n",
    "hidden_layer_1 = layers.Dense(8, activation='relu')(hidden_layer_1)\n",
    "output_tensor_1 = layers.Dense(1)(hidden_layer_1)\n",
    "\n",
    "input_2 = layers.Input(shape=(5,5))\n",
    "input_tensor_2 = layers.LSTM(32, activation='tanh')(input_2)    # x2에 투입할 모델\n",
    "hidden_layer_2 = layers.Dense(16, activation='relu')(input_tensor_2)\n",
    "hidden_layer_2 = layers.Dense(8, activation='relu')(hidden_layer_2)\n",
    "output_tensor_2 = layers.Dense(1)(hidden_layer_2)\n",
    "\n",
    "from keras.layers.merge import concatenate , Add\n",
    "\n",
    "merged_model = concatenate(inputs=[output_tensor_1, output_tensor_2])  # 두 개 이상은 []로 묶기\n",
    "# merged_model = Add()([output_tensor_1, output_tensor_2])  # concatenate 대신 Add를 사용해도 가능하다.\n",
    "\n",
    "output_tensor_3 = layers.Dense(8)(merged_model)        # 첫 번째 아웃풋 모델\n",
    "output_tensor_3 = layers.Dense(1)(output_tensor_3)\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_1, input_2],\n",
    "              outputs=output_tensor_3)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 2021649946.6667 - mae: 44836.9219\n",
      "Epoch 2/100\n",
      "168/168 [==============================] - 0s 889us/step - loss: 2021338818.2857 - mae: 44833.4570\n",
      "Epoch 3/100\n",
      "168/168 [==============================] - 0s 928us/step - loss: 2020168038.8571 - mae: 44820.4062\n",
      "Epoch 4/100\n",
      "168/168 [==============================] - 0s 827us/step - loss: 2016600521.1429 - mae: 44780.6992\n",
      "Epoch 5/100\n",
      "168/168 [==============================] - 0s 873us/step - loss: 2006664112.7619 - mae: 44669.5820\n",
      "Epoch 6/100\n",
      "168/168 [==============================] - 0s 872us/step - loss: 1982012678.0952 - mae: 44393.7695\n",
      "Epoch 7/100\n",
      "168/168 [==============================] - 0s 871us/step - loss: 1935425177.1429 - mae: 43865.2578\n",
      "Epoch 8/100\n",
      "168/168 [==============================] - 0s 863us/step - loss: 1854425938.2857 - mae: 42933.4414\n",
      "Epoch 9/100\n",
      "168/168 [==============================] - 0s 797us/step - loss: 1725092412.9524 - mae: 41391.0312\n",
      "Epoch 10/100\n",
      "168/168 [==============================] - 0s 939us/step - loss: 1534502528.7619 - mae: 39008.7969\n",
      "Epoch 11/100\n",
      "168/168 [==============================] - 0s 834us/step - loss: 1276952262.8571 - mae: 35558.4727\n",
      "Epoch 12/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 964816327.2381 - mae: 30835.8574\n",
      "Epoch 13/100\n",
      "168/168 [==============================] - 0s 864us/step - loss: 632591688.5714 - mae: 24885.9922\n",
      "Epoch 14/100\n",
      "168/168 [==============================] - 0s 924us/step - loss: 340440634.5714 - mae: 18016.0098\n",
      "Epoch 15/100\n",
      "168/168 [==============================] - 0s 1ms/step - loss: 141143551.5952 - mae: 11207.9824\n",
      "Epoch 16/100\n",
      "168/168 [==============================] - 0s 897us/step - loss: 46388295.9807 - mae: 5954.4590\n",
      "Epoch 17/100\n",
      "168/168 [==============================] - 0s 822us/step - loss: 17587467.7560 - mae: 3490.3679\n",
      "Epoch 18/100\n",
      "168/168 [==============================] - 0s 920us/step - loss: 11940320.9360 - mae: 2808.8459\n",
      "Epoch 19/100\n",
      "168/168 [==============================] - 0s 840us/step - loss: 11250473.4866 - mae: 2684.7644\n",
      "Epoch 20/100\n",
      "168/168 [==============================] - 0s 931us/step - loss: 37592698.2768 - mae: 3487.2676\n",
      "Epoch 21/100\n",
      "168/168 [==============================] - 0s 894us/step - loss: 11293633.7009 - mae: 2624.4744\n",
      "Epoch 22/100\n",
      "168/168 [==============================] - 0s 950us/step - loss: 11174813.5722 - mae: 2646.4561\n",
      "Epoch 23/100\n",
      "168/168 [==============================] - 0s 837us/step - loss: 11151113.8110 - mae: 2654.2734\n",
      "Epoch 24/100\n",
      "168/168 [==============================] - 0s 850us/step - loss: 11094423.2202 - mae: 2640.4146\n",
      "Epoch 25/100\n",
      "168/168 [==============================] - 0s 876us/step - loss: 11055752.6436 - mae: 2654.5950\n",
      "Epoch 26/100\n",
      "168/168 [==============================] - 0s 951us/step - loss: 10065633.8378 - mae: 2528.4348\n",
      "Epoch 27/100\n",
      "168/168 [==============================] - 0s 967us/step - loss: 8100818.4784 - mae: 2232.8628\n",
      "Epoch 28/100\n",
      "168/168 [==============================] - 0s 864us/step - loss: 7260689.5195 - mae: 2086.3059\n",
      "Epoch 29/100\n",
      "168/168 [==============================] - 0s 982us/step - loss: 6473025.7679 - mae: 1957.9850\n",
      "Epoch 30/100\n",
      "168/168 [==============================] - 0s 889us/step - loss: 4595537.8904 - mae: 1614.8466\n",
      "Epoch 31/100\n",
      "168/168 [==============================] - 0s 900us/step - loss: 3060323.5616 - mae: 1270.2592\n",
      "Epoch 32/100\n",
      "168/168 [==============================] - 0s 858us/step - loss: 2380473.6178 - mae: 1128.8893\n",
      "Epoch 33/100\n",
      "168/168 [==============================] - 0s 837us/step - loss: 2007914.6114 - mae: 1059.5583\n",
      "Epoch 34/100\n",
      "168/168 [==============================] - 0s 852us/step - loss: 1720273.0894 - mae: 994.9416\n",
      "Epoch 35/100\n",
      "168/168 [==============================] - 0s 896us/step - loss: 1519250.0599 - mae: 952.3278\n",
      "Epoch 36/100\n",
      "168/168 [==============================] - 0s 823us/step - loss: 1379751.0847 - mae: 924.1388\n",
      "Epoch 37/100\n",
      "168/168 [==============================] - 0s 837us/step - loss: 1235893.4997 - mae: 878.8210\n",
      "Epoch 38/100\n",
      "168/168 [==============================] - 0s 900us/step - loss: 1153164.2666 - mae: 862.5015\n",
      "Epoch 39/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 1108655.7457 - mae: 842.8427\n",
      "Epoch 40/100\n",
      "168/168 [==============================] - 0s 875us/step - loss: 1032410.4192 - mae: 802.3674\n",
      "Epoch 41/100\n",
      "168/168 [==============================] - 0s 813us/step - loss: 1004899.4330 - mae: 797.4173\n",
      "Epoch 42/100\n",
      "168/168 [==============================] - 0s 814us/step - loss: 951702.8123 - mae: 784.4030\n",
      "Epoch 43/100\n",
      "168/168 [==============================] - 0s 831us/step - loss: 947213.5347 - mae: 778.8763\n",
      "Epoch 44/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 884685.9169 - mae: 755.5394\n",
      "Epoch 45/100\n",
      "168/168 [==============================] - 0s 774us/step - loss: 872220.5415 - mae: 737.6567\n",
      "Epoch 46/100\n",
      "168/168 [==============================] - 0s 825us/step - loss: 841834.7281 - mae: 736.5089\n",
      "Epoch 47/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 821253.3990 - mae: 718.4382\n",
      "Epoch 48/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 811572.4294 - mae: 709.1886\n",
      "Epoch 49/100\n",
      "168/168 [==============================] - 0s 803us/step - loss: 801778.6115 - mae: 711.7345\n",
      "Epoch 50/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 788339.9688 - mae: 703.9775\n",
      "Epoch 51/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 778392.1982 - mae: 711.0204\n",
      "Epoch 52/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 793466.3620 - mae: 710.9204\n",
      "Epoch 53/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 758008.3339 - mae: 696.5193\n",
      "Epoch 54/100\n",
      "168/168 [==============================] - 0s 781us/step - loss: 754294.0487 - mae: 688.3376\n",
      "Epoch 55/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 736063.8036 - mae: 679.1700\n",
      "Epoch 56/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 734267.6967 - mae: 684.8145\n",
      "Epoch 57/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 720813.7100 - mae: 679.8068\n",
      "Epoch 58/100\n",
      "168/168 [==============================] - 0s 780us/step - loss: 721541.2202 - mae: 670.8558\n",
      "Epoch 59/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 710244.3086 - mae: 666.4990\n",
      "Epoch 60/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 698525.3135 - mae: 661.3942\n",
      "Epoch 61/100\n",
      "168/168 [==============================] - 0s 786us/step - loss: 688363.2669 - mae: 654.3519\n",
      "Epoch 62/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 701846.2934 - mae: 666.4618\n",
      "Epoch 63/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 697426.5289 - mae: 661.8625\n",
      "Epoch 64/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 683200.1712 - mae: 659.4089\n",
      "Epoch 65/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 701047.1387 - mae: 660.2999\n",
      "Epoch 66/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 673854.6226 - mae: 652.1779\n",
      "Epoch 67/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 675932.9392 - mae: 648.6771\n",
      "Epoch 68/100\n",
      "168/168 [==============================] - 0s 787us/step - loss: 668534.9375 - mae: 638.9020\n",
      "Epoch 69/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 655842.0116 - mae: 633.8340\n",
      "Epoch 70/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 662454.7951 - mae: 646.5005\n",
      "Epoch 71/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 682401.9466 - mae: 658.8448\n",
      "Epoch 72/100\n",
      "168/168 [==============================] - 0s 796us/step - loss: 663203.5894 - mae: 644.9453\n",
      "Epoch 73/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 660487.8662 - mae: 635.8332\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 0s 772us/step - loss: 657610.9283 - mae: 638.1492\n",
      "Epoch 75/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 652746.8643 - mae: 640.7409\n",
      "Epoch 76/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 642131.6582 - mae: 627.9054\n",
      "Epoch 77/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 649655.0180 - mae: 634.3304\n",
      "Epoch 78/100\n",
      "168/168 [==============================] - 0s 795us/step - loss: 651232.6185 - mae: 647.3983\n",
      "Epoch 79/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 655411.9464 - mae: 626.9874\n",
      "Epoch 80/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 626489.8661 - mae: 621.3532\n",
      "Epoch 81/100\n",
      "168/168 [==============================] - 0s 790us/step - loss: 658872.7142 - mae: 642.0807\n",
      "Epoch 82/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 639299.9054 - mae: 636.3892\n",
      "Epoch 83/100\n",
      "168/168 [==============================] - 0s 767us/step - loss: 645855.5065 - mae: 628.8093\n",
      "Epoch 84/100\n",
      "168/168 [==============================] - 0s 785us/step - loss: 633304.4684 - mae: 627.2233\n",
      "Epoch 85/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 638204.5838 - mae: 630.9641\n",
      "Epoch 86/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 637186.5366 - mae: 632.8475\n",
      "Epoch 87/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 625124.2537 - mae: 619.7138\n",
      "Epoch 88/100\n",
      "168/168 [==============================] - 0s 766us/step - loss: 608873.6352 - mae: 615.3550\n",
      "Epoch 89/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 635870.2056 - mae: 637.8931\n",
      "Epoch 90/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 626971.3970 - mae: 617.1903\n",
      "Epoch 91/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 613996.0403 - mae: 615.7626\n",
      "Epoch 92/100\n",
      "168/168 [==============================] - 0s 784us/step - loss: 643109.1019 - mae: 620.7689\n",
      "Epoch 93/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 626452.3546 - mae: 616.1474\n",
      "Epoch 94/100\n",
      "168/168 [==============================] - 0s 781us/step - loss: 615746.2699 - mae: 621.7148\n",
      "Epoch 95/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 631991.6134 - mae: 624.4539\n",
      "Epoch 96/100\n",
      "168/168 [==============================] - 0s 779us/step - loss: 600999.3785 - mae: 612.0772\n",
      "Epoch 97/100\n",
      "168/168 [==============================] - 0s 770us/step - loss: 616315.8178 - mae: 608.3489\n",
      "Epoch 98/100\n",
      "168/168 [==============================] - 0s 772us/step - loss: 610808.4028 - mae: 611.1622\n",
      "Epoch 99/100\n",
      "168/168 [==============================] - 0s 778us/step - loss: 638753.2664 - mae: 634.8803\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - 0s 776us/step - loss: 603691.9349 - mae: 605.2312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18389a18320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. 훈련\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "model.fit([x1_train, x2_train], y1_train, epochs=100, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253/253 [==============================] - 0s 910us/step\n",
      "results_acc:  [6087304.974987648, 1353.8358154296875]\n"
     ]
    }
   ],
   "source": [
    "results_acc = model.evaluate([x1_test, x2_test], y1_test, batch_size=5)\n",
    "print('results_acc: ', results_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 2467.246811286235\n",
      "R2 :  0.7212017095123235\n"
     ]
    }
   ],
   "source": [
    "# RMSE 만들기\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_predict = model.predict([x1_test,x2_test], batch_size=1)\n",
    "\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print(\"RMSE :\", RMSE(y1_test, y_predict))\n",
    "\n",
    "# R2 지표 만들기\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_y_predict = r2_score(y1_test, y_predict)\n",
    "print(\"R2 : \", r2_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종가(test) : [42750]\n",
      "종가(pred) : [41994.188]\n",
      "종가(test) : [42150]\n",
      "종가(pred) : [42390.527]\n",
      "종가(test) : [42000]\n",
      "종가(pred) : [42416.77]\n",
      "종가(test) : [43050]\n",
      "종가(pred) : [42058.043]\n",
      "종가(test) : [44750]\n",
      "종가(pred) : [42594.715]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"종가(test) : {}\".format(y1_test[i]))\n",
    "    print(\"종가(pred) : {}\".format(y_predict[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
